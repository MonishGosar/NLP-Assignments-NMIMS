{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a8e12c7-597a-4085-a503-1ba6fdb52c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.24.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.1.0-py3-none-any.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020ec46b-3f9c-4aa4-83d6-736766f93fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d3a327-dec5-4474-a63c-393616ea3222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7b76b8-3c4f-4bbe-afdf-aa44578205d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['question1', 'question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e8e380-8b9d-443f-ac03-e009f83bc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ab8082-1073-4bd1-807b-ea356bc0934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d4efb0-1b21-4e5e-9251-08ccc2239b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['question1'] = df['question1'].apply(preprocess)\n",
    "    df['question2'] = df['question2'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1604586d-50a8-4146-aaf8-bfcac9cdbd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28033f27-ae8f-423c-9b4a-5ed914e5eddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_name = 'all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3dff95b-604a-482f-8006-214caa87898e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1dfbe3a0c44be5adf18d38839a38ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0c261e224a4fc1a8319c909990e13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b771f34adc74021907580570496dab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b410b9a67404001bb4646aefe9f59c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ada39ae0854b76a54c52fee659a61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f8eca61da14b9784e54a952863615e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696e8072f13b4a77a381f2f468235107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f1f70c7d2f4fa695e41d5819349000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5d7eddd0b347c0ab0f080aa83cadab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0adf5cae314091a7f7b500ebd4f2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cbd652a60f4c418371fb15d0a04e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10108' max='10108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10108/10108 16:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cosine Accuracy</th>\n",
       "      <th>Cosine Accuracy Threshold</th>\n",
       "      <th>Cosine F1</th>\n",
       "      <th>Cosine F1 Threshold</th>\n",
       "      <th>Cosine Precision</th>\n",
       "      <th>Cosine Recall</th>\n",
       "      <th>Cosine Ap</th>\n",
       "      <th>Dot Accuracy</th>\n",
       "      <th>Dot Accuracy Threshold</th>\n",
       "      <th>Dot F1</th>\n",
       "      <th>Dot F1 Threshold</th>\n",
       "      <th>Dot Precision</th>\n",
       "      <th>Dot Recall</th>\n",
       "      <th>Dot Ap</th>\n",
       "      <th>Manhattan Accuracy</th>\n",
       "      <th>Manhattan Accuracy Threshold</th>\n",
       "      <th>Manhattan F1</th>\n",
       "      <th>Manhattan F1 Threshold</th>\n",
       "      <th>Manhattan Precision</th>\n",
       "      <th>Manhattan Recall</th>\n",
       "      <th>Manhattan Ap</th>\n",
       "      <th>Euclidean Accuracy</th>\n",
       "      <th>Euclidean Accuracy Threshold</th>\n",
       "      <th>Euclidean F1</th>\n",
       "      <th>Euclidean F1 Threshold</th>\n",
       "      <th>Euclidean Precision</th>\n",
       "      <th>Euclidean Recall</th>\n",
       "      <th>Euclidean Ap</th>\n",
       "      <th>Max Accuracy</th>\n",
       "      <th>Max Accuracy Threshold</th>\n",
       "      <th>Max F1</th>\n",
       "      <th>Max F1 Threshold</th>\n",
       "      <th>Max Precision</th>\n",
       "      <th>Max Recall</th>\n",
       "      <th>Max Ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.608741</td>\n",
       "      <td>0.778616</td>\n",
       "      <td>0.565268</td>\n",
       "      <td>0.710582</td>\n",
       "      <td>0.861057</td>\n",
       "      <td>0.809335</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.608741</td>\n",
       "      <td>0.778616</td>\n",
       "      <td>0.565268</td>\n",
       "      <td>0.710582</td>\n",
       "      <td>0.861057</td>\n",
       "      <td>0.809481</td>\n",
       "      <td>0.824482</td>\n",
       "      <td>13.769339</td>\n",
       "      <td>0.777662</td>\n",
       "      <td>14.285698</td>\n",
       "      <td>0.716098</td>\n",
       "      <td>0.850807</td>\n",
       "      <td>0.809278</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.884601</td>\n",
       "      <td>0.778616</td>\n",
       "      <td>0.932451</td>\n",
       "      <td>0.710582</td>\n",
       "      <td>0.861057</td>\n",
       "      <td>0.809335</td>\n",
       "      <td>0.824482</td>\n",
       "      <td>13.769339</td>\n",
       "      <td>0.778616</td>\n",
       "      <td>14.285698</td>\n",
       "      <td>0.716098</td>\n",
       "      <td>0.861057</td>\n",
       "      <td>0.809481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.624634</td>\n",
       "      <td>0.788207</td>\n",
       "      <td>0.563832</td>\n",
       "      <td>0.728352</td>\n",
       "      <td>0.858779</td>\n",
       "      <td>0.827210</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.624634</td>\n",
       "      <td>0.788207</td>\n",
       "      <td>0.563832</td>\n",
       "      <td>0.728352</td>\n",
       "      <td>0.858779</td>\n",
       "      <td>0.827358</td>\n",
       "      <td>0.834005</td>\n",
       "      <td>13.083944</td>\n",
       "      <td>0.787751</td>\n",
       "      <td>14.402431</td>\n",
       "      <td>0.732355</td>\n",
       "      <td>0.852214</td>\n",
       "      <td>0.827102</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.866448</td>\n",
       "      <td>0.788207</td>\n",
       "      <td>0.933990</td>\n",
       "      <td>0.728352</td>\n",
       "      <td>0.858779</td>\n",
       "      <td>0.827210</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>13.083944</td>\n",
       "      <td>0.788207</td>\n",
       "      <td>14.402431</td>\n",
       "      <td>0.732355</td>\n",
       "      <td>0.858779</td>\n",
       "      <td>0.827358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.842687</td>\n",
       "      <td>0.615121</td>\n",
       "      <td>0.798525</td>\n",
       "      <td>0.560673</td>\n",
       "      <td>0.737733</td>\n",
       "      <td>0.870235</td>\n",
       "      <td>0.839943</td>\n",
       "      <td>0.842687</td>\n",
       "      <td>0.615122</td>\n",
       "      <td>0.798525</td>\n",
       "      <td>0.560673</td>\n",
       "      <td>0.737733</td>\n",
       "      <td>0.870235</td>\n",
       "      <td>0.839812</td>\n",
       "      <td>0.842737</td>\n",
       "      <td>13.505814</td>\n",
       "      <td>0.798264</td>\n",
       "      <td>14.406073</td>\n",
       "      <td>0.743058</td>\n",
       "      <td>0.862330</td>\n",
       "      <td>0.839850</td>\n",
       "      <td>0.842687</td>\n",
       "      <td>0.877358</td>\n",
       "      <td>0.798525</td>\n",
       "      <td>0.937366</td>\n",
       "      <td>0.737733</td>\n",
       "      <td>0.870235</td>\n",
       "      <td>0.839943</td>\n",
       "      <td>0.842737</td>\n",
       "      <td>13.505814</td>\n",
       "      <td>0.798525</td>\n",
       "      <td>14.406073</td>\n",
       "      <td>0.743058</td>\n",
       "      <td>0.870235</td>\n",
       "      <td>0.839943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.846867</td>\n",
       "      <td>0.633864</td>\n",
       "      <td>0.802154</td>\n",
       "      <td>0.580140</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>0.863335</td>\n",
       "      <td>0.846559</td>\n",
       "      <td>0.846867</td>\n",
       "      <td>0.633864</td>\n",
       "      <td>0.802154</td>\n",
       "      <td>0.580140</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>0.863335</td>\n",
       "      <td>0.846506</td>\n",
       "      <td>0.846694</td>\n",
       "      <td>13.003998</td>\n",
       "      <td>0.802105</td>\n",
       "      <td>14.363365</td>\n",
       "      <td>0.745441</td>\n",
       "      <td>0.868091</td>\n",
       "      <td>0.846472</td>\n",
       "      <td>0.846867</td>\n",
       "      <td>0.855728</td>\n",
       "      <td>0.802154</td>\n",
       "      <td>0.916362</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>0.863335</td>\n",
       "      <td>0.846559</td>\n",
       "      <td>0.846867</td>\n",
       "      <td>13.003998</td>\n",
       "      <td>0.802154</td>\n",
       "      <td>14.363365</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>0.868091</td>\n",
       "      <td>0.846559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850281</td>\n",
       "      <td>0.633182</td>\n",
       "      <td>0.805311</td>\n",
       "      <td>0.574952</td>\n",
       "      <td>0.749899</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.851162</td>\n",
       "      <td>0.850281</td>\n",
       "      <td>0.633182</td>\n",
       "      <td>0.805311</td>\n",
       "      <td>0.574952</td>\n",
       "      <td>0.749899</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.851186</td>\n",
       "      <td>0.850157</td>\n",
       "      <td>13.362518</td>\n",
       "      <td>0.805708</td>\n",
       "      <td>14.255762</td>\n",
       "      <td>0.753102</td>\n",
       "      <td>0.866216</td>\n",
       "      <td>0.851047</td>\n",
       "      <td>0.850281</td>\n",
       "      <td>0.856525</td>\n",
       "      <td>0.805311</td>\n",
       "      <td>0.922007</td>\n",
       "      <td>0.749899</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.851162</td>\n",
       "      <td>0.850281</td>\n",
       "      <td>13.362518</td>\n",
       "      <td>0.805708</td>\n",
       "      <td>14.255762</td>\n",
       "      <td>0.753102</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.851186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.852358</td>\n",
       "      <td>0.638371</td>\n",
       "      <td>0.810005</td>\n",
       "      <td>0.582600</td>\n",
       "      <td>0.760284</td>\n",
       "      <td>0.866685</td>\n",
       "      <td>0.853934</td>\n",
       "      <td>0.852358</td>\n",
       "      <td>0.638371</td>\n",
       "      <td>0.810005</td>\n",
       "      <td>0.582600</td>\n",
       "      <td>0.760284</td>\n",
       "      <td>0.866685</td>\n",
       "      <td>0.853995</td>\n",
       "      <td>0.852408</td>\n",
       "      <td>13.297185</td>\n",
       "      <td>0.810160</td>\n",
       "      <td>14.174862</td>\n",
       "      <td>0.761540</td>\n",
       "      <td>0.865412</td>\n",
       "      <td>0.853843</td>\n",
       "      <td>0.852358</td>\n",
       "      <td>0.850445</td>\n",
       "      <td>0.810005</td>\n",
       "      <td>0.913674</td>\n",
       "      <td>0.760284</td>\n",
       "      <td>0.866685</td>\n",
       "      <td>0.853934</td>\n",
       "      <td>0.852408</td>\n",
       "      <td>13.297185</td>\n",
       "      <td>0.810160</td>\n",
       "      <td>14.174862</td>\n",
       "      <td>0.761540</td>\n",
       "      <td>0.866685</td>\n",
       "      <td>0.853995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.853274</td>\n",
       "      <td>0.641741</td>\n",
       "      <td>0.810531</td>\n",
       "      <td>0.586685</td>\n",
       "      <td>0.761574</td>\n",
       "      <td>0.866216</td>\n",
       "      <td>0.856864</td>\n",
       "      <td>0.853274</td>\n",
       "      <td>0.641741</td>\n",
       "      <td>0.810531</td>\n",
       "      <td>0.586685</td>\n",
       "      <td>0.761574</td>\n",
       "      <td>0.866216</td>\n",
       "      <td>0.856886</td>\n",
       "      <td>0.853076</td>\n",
       "      <td>13.390553</td>\n",
       "      <td>0.810767</td>\n",
       "      <td>14.279335</td>\n",
       "      <td>0.757789</td>\n",
       "      <td>0.871709</td>\n",
       "      <td>0.856782</td>\n",
       "      <td>0.853274</td>\n",
       "      <td>0.846474</td>\n",
       "      <td>0.810531</td>\n",
       "      <td>0.909192</td>\n",
       "      <td>0.761574</td>\n",
       "      <td>0.866216</td>\n",
       "      <td>0.856864</td>\n",
       "      <td>0.853274</td>\n",
       "      <td>13.390553</td>\n",
       "      <td>0.810767</td>\n",
       "      <td>14.279335</td>\n",
       "      <td>0.761574</td>\n",
       "      <td>0.871709</td>\n",
       "      <td>0.856886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.854312</td>\n",
       "      <td>0.637162</td>\n",
       "      <td>0.813042</td>\n",
       "      <td>0.567774</td>\n",
       "      <td>0.755887</td>\n",
       "      <td>0.879547</td>\n",
       "      <td>0.858150</td>\n",
       "      <td>0.854312</td>\n",
       "      <td>0.637162</td>\n",
       "      <td>0.813042</td>\n",
       "      <td>0.567775</td>\n",
       "      <td>0.755887</td>\n",
       "      <td>0.879547</td>\n",
       "      <td>0.858213</td>\n",
       "      <td>0.854560</td>\n",
       "      <td>13.422560</td>\n",
       "      <td>0.812377</td>\n",
       "      <td>14.546280</td>\n",
       "      <td>0.752873</td>\n",
       "      <td>0.882093</td>\n",
       "      <td>0.858045</td>\n",
       "      <td>0.854312</td>\n",
       "      <td>0.851866</td>\n",
       "      <td>0.813042</td>\n",
       "      <td>0.929759</td>\n",
       "      <td>0.755887</td>\n",
       "      <td>0.879547</td>\n",
       "      <td>0.858150</td>\n",
       "      <td>0.854560</td>\n",
       "      <td>13.422560</td>\n",
       "      <td>0.813042</td>\n",
       "      <td>14.546280</td>\n",
       "      <td>0.755887</td>\n",
       "      <td>0.882093</td>\n",
       "      <td>0.858213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.854807</td>\n",
       "      <td>0.626865</td>\n",
       "      <td>0.812749</td>\n",
       "      <td>0.568649</td>\n",
       "      <td>0.757718</td>\n",
       "      <td>0.876398</td>\n",
       "      <td>0.858691</td>\n",
       "      <td>0.854807</td>\n",
       "      <td>0.626865</td>\n",
       "      <td>0.812749</td>\n",
       "      <td>0.568649</td>\n",
       "      <td>0.757718</td>\n",
       "      <td>0.876398</td>\n",
       "      <td>0.858920</td>\n",
       "      <td>0.854634</td>\n",
       "      <td>12.910105</td>\n",
       "      <td>0.812626</td>\n",
       "      <td>14.171444</td>\n",
       "      <td>0.766277</td>\n",
       "      <td>0.864943</td>\n",
       "      <td>0.858604</td>\n",
       "      <td>0.854807</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>0.812749</td>\n",
       "      <td>0.928817</td>\n",
       "      <td>0.757718</td>\n",
       "      <td>0.876398</td>\n",
       "      <td>0.858691</td>\n",
       "      <td>0.854807</td>\n",
       "      <td>12.910105</td>\n",
       "      <td>0.812749</td>\n",
       "      <td>14.171444</td>\n",
       "      <td>0.766277</td>\n",
       "      <td>0.876398</td>\n",
       "      <td>0.858920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.855599</td>\n",
       "      <td>0.631488</td>\n",
       "      <td>0.813064</td>\n",
       "      <td>0.566381</td>\n",
       "      <td>0.758869</td>\n",
       "      <td>0.875595</td>\n",
       "      <td>0.859405</td>\n",
       "      <td>0.855599</td>\n",
       "      <td>0.631488</td>\n",
       "      <td>0.813064</td>\n",
       "      <td>0.566381</td>\n",
       "      <td>0.758869</td>\n",
       "      <td>0.875595</td>\n",
       "      <td>0.859349</td>\n",
       "      <td>0.855401</td>\n",
       "      <td>13.187016</td>\n",
       "      <td>0.812794</td>\n",
       "      <td>14.301113</td>\n",
       "      <td>0.764016</td>\n",
       "      <td>0.868225</td>\n",
       "      <td>0.859307</td>\n",
       "      <td>0.855599</td>\n",
       "      <td>0.858501</td>\n",
       "      <td>0.813064</td>\n",
       "      <td>0.931256</td>\n",
       "      <td>0.758869</td>\n",
       "      <td>0.875595</td>\n",
       "      <td>0.859405</td>\n",
       "      <td>0.855599</td>\n",
       "      <td>13.187016</td>\n",
       "      <td>0.813064</td>\n",
       "      <td>14.301113</td>\n",
       "      <td>0.764016</td>\n",
       "      <td>0.875595</td>\n",
       "      <td>0.859405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10108</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.855524</td>\n",
       "      <td>0.631465</td>\n",
       "      <td>0.813058</td>\n",
       "      <td>0.565677</td>\n",
       "      <td>0.758557</td>\n",
       "      <td>0.875997</td>\n",
       "      <td>0.859422</td>\n",
       "      <td>0.855524</td>\n",
       "      <td>0.631465</td>\n",
       "      <td>0.813058</td>\n",
       "      <td>0.565677</td>\n",
       "      <td>0.758557</td>\n",
       "      <td>0.875997</td>\n",
       "      <td>0.859600</td>\n",
       "      <td>0.855450</td>\n",
       "      <td>13.175864</td>\n",
       "      <td>0.812804</td>\n",
       "      <td>14.167281</td>\n",
       "      <td>0.767860</td>\n",
       "      <td>0.863335</td>\n",
       "      <td>0.859327</td>\n",
       "      <td>0.855524</td>\n",
       "      <td>0.858527</td>\n",
       "      <td>0.813058</td>\n",
       "      <td>0.932011</td>\n",
       "      <td>0.758557</td>\n",
       "      <td>0.875997</td>\n",
       "      <td>0.859422</td>\n",
       "      <td>0.855524</td>\n",
       "      <td>13.175864</td>\n",
       "      <td>0.813058</td>\n",
       "      <td>14.167281</td>\n",
       "      <td>0.767860</td>\n",
       "      <td>0.875997</td>\n",
       "      <td>0.859600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses, InputExample, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "train_examples = [InputExample(texts=[row['question1'], row['question2']], label=float(row['is_duplicate']))\n",
    "                  for index, row in train_df.iterrows()]\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "\n",
    "bi_encoder = SentenceTransformer(model_name, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "loss = losses.CosineSimilarityLoss(model=bi_encoder)\n",
    "\n",
    "evaluator = evaluation.BinaryClassificationEvaluator(\n",
    "    sentences1=val_df['question1'].tolist(),\n",
    "    sentences2=val_df['question2'].tolist(),\n",
    "    labels=val_df['is_duplicate'].astype(float).tolist()\n",
    ")\n",
    "\n",
    "bi_encoder.fit(\n",
    "    train_objectives=[(train_dataloader, loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=1,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=100,\n",
    "    output_path='output/bi_encoder_cosine',\n",
    "    use_amp=True  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c573d384-aa57-44f0-a143-b086ad34a647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10108' max='10108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10108/10108 16:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cosine Accuracy</th>\n",
       "      <th>Cosine Accuracy Threshold</th>\n",
       "      <th>Cosine F1</th>\n",
       "      <th>Cosine F1 Threshold</th>\n",
       "      <th>Cosine Precision</th>\n",
       "      <th>Cosine Recall</th>\n",
       "      <th>Cosine Ap</th>\n",
       "      <th>Dot Accuracy</th>\n",
       "      <th>Dot Accuracy Threshold</th>\n",
       "      <th>Dot F1</th>\n",
       "      <th>Dot F1 Threshold</th>\n",
       "      <th>Dot Precision</th>\n",
       "      <th>Dot Recall</th>\n",
       "      <th>Dot Ap</th>\n",
       "      <th>Manhattan Accuracy</th>\n",
       "      <th>Manhattan Accuracy Threshold</th>\n",
       "      <th>Manhattan F1</th>\n",
       "      <th>Manhattan F1 Threshold</th>\n",
       "      <th>Manhattan Precision</th>\n",
       "      <th>Manhattan Recall</th>\n",
       "      <th>Manhattan Ap</th>\n",
       "      <th>Euclidean Accuracy</th>\n",
       "      <th>Euclidean Accuracy Threshold</th>\n",
       "      <th>Euclidean F1</th>\n",
       "      <th>Euclidean F1 Threshold</th>\n",
       "      <th>Euclidean Precision</th>\n",
       "      <th>Euclidean Recall</th>\n",
       "      <th>Euclidean Ap</th>\n",
       "      <th>Max Accuracy</th>\n",
       "      <th>Max Accuracy Threshold</th>\n",
       "      <th>Max F1</th>\n",
       "      <th>Max F1 Threshold</th>\n",
       "      <th>Max Precision</th>\n",
       "      <th>Max Recall</th>\n",
       "      <th>Max Ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.868634</td>\n",
       "      <td>0.815661</td>\n",
       "      <td>0.827606</td>\n",
       "      <td>0.772353</td>\n",
       "      <td>0.779145</td>\n",
       "      <td>0.882495</td>\n",
       "      <td>0.875703</td>\n",
       "      <td>0.868634</td>\n",
       "      <td>0.815662</td>\n",
       "      <td>0.827606</td>\n",
       "      <td>0.772353</td>\n",
       "      <td>0.779145</td>\n",
       "      <td>0.882495</td>\n",
       "      <td>0.875716</td>\n",
       "      <td>0.868189</td>\n",
       "      <td>9.461376</td>\n",
       "      <td>0.827612</td>\n",
       "      <td>10.563437</td>\n",
       "      <td>0.776512</td>\n",
       "      <td>0.885911</td>\n",
       "      <td>0.875620</td>\n",
       "      <td>0.868634</td>\n",
       "      <td>0.607188</td>\n",
       "      <td>0.827606</td>\n",
       "      <td>0.674755</td>\n",
       "      <td>0.779145</td>\n",
       "      <td>0.882495</td>\n",
       "      <td>0.875703</td>\n",
       "      <td>0.868634</td>\n",
       "      <td>9.461376</td>\n",
       "      <td>0.827612</td>\n",
       "      <td>10.563437</td>\n",
       "      <td>0.779145</td>\n",
       "      <td>0.885911</td>\n",
       "      <td>0.875716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.870341</td>\n",
       "      <td>0.811458</td>\n",
       "      <td>0.831503</td>\n",
       "      <td>0.768637</td>\n",
       "      <td>0.776351</td>\n",
       "      <td>0.895089</td>\n",
       "      <td>0.879548</td>\n",
       "      <td>0.870341</td>\n",
       "      <td>0.811458</td>\n",
       "      <td>0.831503</td>\n",
       "      <td>0.768637</td>\n",
       "      <td>0.776351</td>\n",
       "      <td>0.895089</td>\n",
       "      <td>0.879350</td>\n",
       "      <td>0.870217</td>\n",
       "      <td>9.464338</td>\n",
       "      <td>0.831498</td>\n",
       "      <td>10.622085</td>\n",
       "      <td>0.774387</td>\n",
       "      <td>0.897702</td>\n",
       "      <td>0.879433</td>\n",
       "      <td>0.870341</td>\n",
       "      <td>0.614071</td>\n",
       "      <td>0.831503</td>\n",
       "      <td>0.680240</td>\n",
       "      <td>0.776351</td>\n",
       "      <td>0.895089</td>\n",
       "      <td>0.879548</td>\n",
       "      <td>0.870341</td>\n",
       "      <td>9.464338</td>\n",
       "      <td>0.831503</td>\n",
       "      <td>10.622085</td>\n",
       "      <td>0.776351</td>\n",
       "      <td>0.897702</td>\n",
       "      <td>0.879548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.874694</td>\n",
       "      <td>0.803666</td>\n",
       "      <td>0.837867</td>\n",
       "      <td>0.775873</td>\n",
       "      <td>0.783926</td>\n",
       "      <td>0.899779</td>\n",
       "      <td>0.883505</td>\n",
       "      <td>0.874694</td>\n",
       "      <td>0.803666</td>\n",
       "      <td>0.837867</td>\n",
       "      <td>0.775873</td>\n",
       "      <td>0.783926</td>\n",
       "      <td>0.899779</td>\n",
       "      <td>0.883322</td>\n",
       "      <td>0.874743</td>\n",
       "      <td>9.617493</td>\n",
       "      <td>0.837593</td>\n",
       "      <td>10.462124</td>\n",
       "      <td>0.782333</td>\n",
       "      <td>0.901253</td>\n",
       "      <td>0.883422</td>\n",
       "      <td>0.874694</td>\n",
       "      <td>0.626632</td>\n",
       "      <td>0.837867</td>\n",
       "      <td>0.669517</td>\n",
       "      <td>0.783926</td>\n",
       "      <td>0.899779</td>\n",
       "      <td>0.883505</td>\n",
       "      <td>0.874743</td>\n",
       "      <td>9.617493</td>\n",
       "      <td>0.837867</td>\n",
       "      <td>10.462124</td>\n",
       "      <td>0.783926</td>\n",
       "      <td>0.901253</td>\n",
       "      <td>0.883505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.875560</td>\n",
       "      <td>0.802413</td>\n",
       "      <td>0.838087</td>\n",
       "      <td>0.779295</td>\n",
       "      <td>0.793711</td>\n",
       "      <td>0.887720</td>\n",
       "      <td>0.886592</td>\n",
       "      <td>0.875560</td>\n",
       "      <td>0.802413</td>\n",
       "      <td>0.838087</td>\n",
       "      <td>0.779295</td>\n",
       "      <td>0.793711</td>\n",
       "      <td>0.887720</td>\n",
       "      <td>0.886764</td>\n",
       "      <td>0.875832</td>\n",
       "      <td>9.815351</td>\n",
       "      <td>0.838244</td>\n",
       "      <td>10.641382</td>\n",
       "      <td>0.782561</td>\n",
       "      <td>0.902459</td>\n",
       "      <td>0.886533</td>\n",
       "      <td>0.875560</td>\n",
       "      <td>0.628629</td>\n",
       "      <td>0.838087</td>\n",
       "      <td>0.664387</td>\n",
       "      <td>0.793711</td>\n",
       "      <td>0.887720</td>\n",
       "      <td>0.886592</td>\n",
       "      <td>0.875832</td>\n",
       "      <td>9.815351</td>\n",
       "      <td>0.838244</td>\n",
       "      <td>10.641382</td>\n",
       "      <td>0.793711</td>\n",
       "      <td>0.902459</td>\n",
       "      <td>0.886764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.877019</td>\n",
       "      <td>0.803816</td>\n",
       "      <td>0.841085</td>\n",
       "      <td>0.781862</td>\n",
       "      <td>0.795449</td>\n",
       "      <td>0.892276</td>\n",
       "      <td>0.887481</td>\n",
       "      <td>0.877019</td>\n",
       "      <td>0.803816</td>\n",
       "      <td>0.841085</td>\n",
       "      <td>0.781862</td>\n",
       "      <td>0.795449</td>\n",
       "      <td>0.892276</td>\n",
       "      <td>0.887535</td>\n",
       "      <td>0.877044</td>\n",
       "      <td>9.509559</td>\n",
       "      <td>0.841043</td>\n",
       "      <td>10.485172</td>\n",
       "      <td>0.787863</td>\n",
       "      <td>0.901923</td>\n",
       "      <td>0.887470</td>\n",
       "      <td>0.877019</td>\n",
       "      <td>0.626393</td>\n",
       "      <td>0.841085</td>\n",
       "      <td>0.660512</td>\n",
       "      <td>0.795449</td>\n",
       "      <td>0.892276</td>\n",
       "      <td>0.887481</td>\n",
       "      <td>0.877044</td>\n",
       "      <td>9.509559</td>\n",
       "      <td>0.841085</td>\n",
       "      <td>10.485172</td>\n",
       "      <td>0.795449</td>\n",
       "      <td>0.901923</td>\n",
       "      <td>0.887535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.878651</td>\n",
       "      <td>0.799904</td>\n",
       "      <td>0.841640</td>\n",
       "      <td>0.785661</td>\n",
       "      <td>0.800326</td>\n",
       "      <td>0.887452</td>\n",
       "      <td>0.889829</td>\n",
       "      <td>0.878651</td>\n",
       "      <td>0.799904</td>\n",
       "      <td>0.841640</td>\n",
       "      <td>0.785661</td>\n",
       "      <td>0.800326</td>\n",
       "      <td>0.887452</td>\n",
       "      <td>0.889903</td>\n",
       "      <td>0.878107</td>\n",
       "      <td>9.845848</td>\n",
       "      <td>0.841873</td>\n",
       "      <td>10.040579</td>\n",
       "      <td>0.805496</td>\n",
       "      <td>0.881691</td>\n",
       "      <td>0.889845</td>\n",
       "      <td>0.878651</td>\n",
       "      <td>0.632608</td>\n",
       "      <td>0.841640</td>\n",
       "      <td>0.654735</td>\n",
       "      <td>0.800326</td>\n",
       "      <td>0.887452</td>\n",
       "      <td>0.889829</td>\n",
       "      <td>0.878651</td>\n",
       "      <td>9.845848</td>\n",
       "      <td>0.841873</td>\n",
       "      <td>10.040579</td>\n",
       "      <td>0.805496</td>\n",
       "      <td>0.887452</td>\n",
       "      <td>0.889903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.879765</td>\n",
       "      <td>0.816348</td>\n",
       "      <td>0.843940</td>\n",
       "      <td>0.797419</td>\n",
       "      <td>0.804826</td>\n",
       "      <td>0.887050</td>\n",
       "      <td>0.890817</td>\n",
       "      <td>0.879765</td>\n",
       "      <td>0.816348</td>\n",
       "      <td>0.843940</td>\n",
       "      <td>0.797419</td>\n",
       "      <td>0.804826</td>\n",
       "      <td>0.887050</td>\n",
       "      <td>0.891089</td>\n",
       "      <td>0.879542</td>\n",
       "      <td>9.283292</td>\n",
       "      <td>0.843849</td>\n",
       "      <td>10.140768</td>\n",
       "      <td>0.794989</td>\n",
       "      <td>0.899109</td>\n",
       "      <td>0.890817</td>\n",
       "      <td>0.879765</td>\n",
       "      <td>0.606056</td>\n",
       "      <td>0.843940</td>\n",
       "      <td>0.636523</td>\n",
       "      <td>0.804826</td>\n",
       "      <td>0.887050</td>\n",
       "      <td>0.890817</td>\n",
       "      <td>0.879765</td>\n",
       "      <td>9.283292</td>\n",
       "      <td>0.843940</td>\n",
       "      <td>10.140768</td>\n",
       "      <td>0.804826</td>\n",
       "      <td>0.899109</td>\n",
       "      <td>0.891089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.880655</td>\n",
       "      <td>0.806673</td>\n",
       "      <td>0.845270</td>\n",
       "      <td>0.780293</td>\n",
       "      <td>0.799474</td>\n",
       "      <td>0.896630</td>\n",
       "      <td>0.892171</td>\n",
       "      <td>0.880655</td>\n",
       "      <td>0.806673</td>\n",
       "      <td>0.845270</td>\n",
       "      <td>0.780293</td>\n",
       "      <td>0.799474</td>\n",
       "      <td>0.896630</td>\n",
       "      <td>0.892066</td>\n",
       "      <td>0.880408</td>\n",
       "      <td>9.880693</td>\n",
       "      <td>0.845082</td>\n",
       "      <td>10.452726</td>\n",
       "      <td>0.793426</td>\n",
       "      <td>0.903932</td>\n",
       "      <td>0.892145</td>\n",
       "      <td>0.880655</td>\n",
       "      <td>0.621815</td>\n",
       "      <td>0.845270</td>\n",
       "      <td>0.662884</td>\n",
       "      <td>0.799474</td>\n",
       "      <td>0.896630</td>\n",
       "      <td>0.892171</td>\n",
       "      <td>0.880655</td>\n",
       "      <td>9.880693</td>\n",
       "      <td>0.845270</td>\n",
       "      <td>10.452726</td>\n",
       "      <td>0.799474</td>\n",
       "      <td>0.903932</td>\n",
       "      <td>0.892171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.880828</td>\n",
       "      <td>0.805342</td>\n",
       "      <td>0.845698</td>\n",
       "      <td>0.778597</td>\n",
       "      <td>0.794617</td>\n",
       "      <td>0.903798</td>\n",
       "      <td>0.893047</td>\n",
       "      <td>0.880828</td>\n",
       "      <td>0.805342</td>\n",
       "      <td>0.845698</td>\n",
       "      <td>0.778597</td>\n",
       "      <td>0.794617</td>\n",
       "      <td>0.903798</td>\n",
       "      <td>0.893071</td>\n",
       "      <td>0.880853</td>\n",
       "      <td>9.765572</td>\n",
       "      <td>0.845435</td>\n",
       "      <td>10.321465</td>\n",
       "      <td>0.795137</td>\n",
       "      <td>0.902526</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.880828</td>\n",
       "      <td>0.623952</td>\n",
       "      <td>0.845698</td>\n",
       "      <td>0.665437</td>\n",
       "      <td>0.794617</td>\n",
       "      <td>0.903798</td>\n",
       "      <td>0.893047</td>\n",
       "      <td>0.880853</td>\n",
       "      <td>9.765572</td>\n",
       "      <td>0.845698</td>\n",
       "      <td>10.321465</td>\n",
       "      <td>0.795137</td>\n",
       "      <td>0.903798</td>\n",
       "      <td>0.893071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>0.801594</td>\n",
       "      <td>0.846253</td>\n",
       "      <td>0.776190</td>\n",
       "      <td>0.797475</td>\n",
       "      <td>0.901387</td>\n",
       "      <td>0.893696</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>0.801594</td>\n",
       "      <td>0.846253</td>\n",
       "      <td>0.776190</td>\n",
       "      <td>0.797475</td>\n",
       "      <td>0.901387</td>\n",
       "      <td>0.893816</td>\n",
       "      <td>0.880803</td>\n",
       "      <td>9.918058</td>\n",
       "      <td>0.846285</td>\n",
       "      <td>10.333284</td>\n",
       "      <td>0.800119</td>\n",
       "      <td>0.898104</td>\n",
       "      <td>0.893667</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>0.629929</td>\n",
       "      <td>0.846253</td>\n",
       "      <td>0.669045</td>\n",
       "      <td>0.797475</td>\n",
       "      <td>0.901387</td>\n",
       "      <td>0.893696</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>9.918058</td>\n",
       "      <td>0.846285</td>\n",
       "      <td>10.333284</td>\n",
       "      <td>0.800119</td>\n",
       "      <td>0.901387</td>\n",
       "      <td>0.893816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10108</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>0.811785</td>\n",
       "      <td>0.846255</td>\n",
       "      <td>0.776203</td>\n",
       "      <td>0.797322</td>\n",
       "      <td>0.901588</td>\n",
       "      <td>0.893706</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>0.811785</td>\n",
       "      <td>0.846241</td>\n",
       "      <td>0.776481</td>\n",
       "      <td>0.797664</td>\n",
       "      <td>0.901119</td>\n",
       "      <td>0.893769</td>\n",
       "      <td>0.880803</td>\n",
       "      <td>9.547161</td>\n",
       "      <td>0.846358</td>\n",
       "      <td>10.329275</td>\n",
       "      <td>0.800143</td>\n",
       "      <td>0.898238</td>\n",
       "      <td>0.893682</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>0.613538</td>\n",
       "      <td>0.846241</td>\n",
       "      <td>0.668609</td>\n",
       "      <td>0.797664</td>\n",
       "      <td>0.901119</td>\n",
       "      <td>0.893706</td>\n",
       "      <td>0.880878</td>\n",
       "      <td>9.547161</td>\n",
       "      <td>0.846358</td>\n",
       "      <td>10.329275</td>\n",
       "      <td>0.800143</td>\n",
       "      <td>0.901588</td>\n",
       "      <td>0.893769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers.losses import ContrastiveLoss\n",
    "\n",
    "contrastive_loss = ContrastiveLoss(model=bi_encoder)\n",
    "\n",
    "bi_encoder.fit(\n",
    "    train_objectives=[(train_dataloader, contrastive_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=1,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=100,\n",
    "    output_path='output/bi_encoder_contrastive',\n",
    "    use_amp=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c81bfc86-f8a4-4135-a411-496ecd3624aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3732' max='3732' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3732/3732 06:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cosine Accuracy</th>\n",
       "      <th>Cosine Accuracy Threshold</th>\n",
       "      <th>Cosine F1</th>\n",
       "      <th>Cosine F1 Threshold</th>\n",
       "      <th>Cosine Precision</th>\n",
       "      <th>Cosine Recall</th>\n",
       "      <th>Cosine Ap</th>\n",
       "      <th>Dot Accuracy</th>\n",
       "      <th>Dot Accuracy Threshold</th>\n",
       "      <th>Dot F1</th>\n",
       "      <th>Dot F1 Threshold</th>\n",
       "      <th>Dot Precision</th>\n",
       "      <th>Dot Recall</th>\n",
       "      <th>Dot Ap</th>\n",
       "      <th>Manhattan Accuracy</th>\n",
       "      <th>Manhattan Accuracy Threshold</th>\n",
       "      <th>Manhattan F1</th>\n",
       "      <th>Manhattan F1 Threshold</th>\n",
       "      <th>Manhattan Precision</th>\n",
       "      <th>Manhattan Recall</th>\n",
       "      <th>Manhattan Ap</th>\n",
       "      <th>Euclidean Accuracy</th>\n",
       "      <th>Euclidean Accuracy Threshold</th>\n",
       "      <th>Euclidean F1</th>\n",
       "      <th>Euclidean F1 Threshold</th>\n",
       "      <th>Euclidean Precision</th>\n",
       "      <th>Euclidean Recall</th>\n",
       "      <th>Euclidean Ap</th>\n",
       "      <th>Max Accuracy</th>\n",
       "      <th>Max Accuracy Threshold</th>\n",
       "      <th>Max F1</th>\n",
       "      <th>Max F1 Threshold</th>\n",
       "      <th>Max Precision</th>\n",
       "      <th>Max Recall</th>\n",
       "      <th>Max Ap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.853496</td>\n",
       "      <td>0.795935</td>\n",
       "      <td>0.814339</td>\n",
       "      <td>0.768413</td>\n",
       "      <td>0.748736</td>\n",
       "      <td>0.892544</td>\n",
       "      <td>0.855852</td>\n",
       "      <td>0.853496</td>\n",
       "      <td>0.795935</td>\n",
       "      <td>0.814339</td>\n",
       "      <td>0.768413</td>\n",
       "      <td>0.748736</td>\n",
       "      <td>0.892544</td>\n",
       "      <td>0.855958</td>\n",
       "      <td>0.852927</td>\n",
       "      <td>9.849564</td>\n",
       "      <td>0.814518</td>\n",
       "      <td>10.568998</td>\n",
       "      <td>0.749746</td>\n",
       "      <td>0.891539</td>\n",
       "      <td>0.855734</td>\n",
       "      <td>0.853496</td>\n",
       "      <td>0.638851</td>\n",
       "      <td>0.814339</td>\n",
       "      <td>0.680569</td>\n",
       "      <td>0.748736</td>\n",
       "      <td>0.892544</td>\n",
       "      <td>0.855852</td>\n",
       "      <td>0.853496</td>\n",
       "      <td>9.849564</td>\n",
       "      <td>0.814518</td>\n",
       "      <td>10.568998</td>\n",
       "      <td>0.749746</td>\n",
       "      <td>0.892544</td>\n",
       "      <td>0.855958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.849737</td>\n",
       "      <td>0.804366</td>\n",
       "      <td>0.808926</td>\n",
       "      <td>0.775373</td>\n",
       "      <td>0.749971</td>\n",
       "      <td>0.877939</td>\n",
       "      <td>0.850831</td>\n",
       "      <td>0.849737</td>\n",
       "      <td>0.804366</td>\n",
       "      <td>0.808926</td>\n",
       "      <td>0.775373</td>\n",
       "      <td>0.749971</td>\n",
       "      <td>0.877939</td>\n",
       "      <td>0.850842</td>\n",
       "      <td>0.849588</td>\n",
       "      <td>9.681833</td>\n",
       "      <td>0.807875</td>\n",
       "      <td>10.513803</td>\n",
       "      <td>0.744938</td>\n",
       "      <td>0.882428</td>\n",
       "      <td>0.850732</td>\n",
       "      <td>0.849737</td>\n",
       "      <td>0.625514</td>\n",
       "      <td>0.808926</td>\n",
       "      <td>0.670264</td>\n",
       "      <td>0.749971</td>\n",
       "      <td>0.877939</td>\n",
       "      <td>0.850831</td>\n",
       "      <td>0.849737</td>\n",
       "      <td>9.681833</td>\n",
       "      <td>0.808926</td>\n",
       "      <td>10.513803</td>\n",
       "      <td>0.749971</td>\n",
       "      <td>0.882428</td>\n",
       "      <td>0.850842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850652</td>\n",
       "      <td>0.808593</td>\n",
       "      <td>0.809071</td>\n",
       "      <td>0.781741</td>\n",
       "      <td>0.750760</td>\n",
       "      <td>0.877202</td>\n",
       "      <td>0.850642</td>\n",
       "      <td>0.850652</td>\n",
       "      <td>0.808593</td>\n",
       "      <td>0.809071</td>\n",
       "      <td>0.781741</td>\n",
       "      <td>0.750760</td>\n",
       "      <td>0.877202</td>\n",
       "      <td>0.850916</td>\n",
       "      <td>0.850627</td>\n",
       "      <td>9.682549</td>\n",
       "      <td>0.808911</td>\n",
       "      <td>10.248860</td>\n",
       "      <td>0.751567</td>\n",
       "      <td>0.875729</td>\n",
       "      <td>0.850541</td>\n",
       "      <td>0.850652</td>\n",
       "      <td>0.618720</td>\n",
       "      <td>0.809071</td>\n",
       "      <td>0.660694</td>\n",
       "      <td>0.750760</td>\n",
       "      <td>0.877202</td>\n",
       "      <td>0.850642</td>\n",
       "      <td>0.850652</td>\n",
       "      <td>9.682549</td>\n",
       "      <td>0.809071</td>\n",
       "      <td>10.248860</td>\n",
       "      <td>0.751567</td>\n",
       "      <td>0.877202</td>\n",
       "      <td>0.850916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3732</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850380</td>\n",
       "      <td>0.808482</td>\n",
       "      <td>0.807444</td>\n",
       "      <td>0.781889</td>\n",
       "      <td>0.748598</td>\n",
       "      <td>0.876331</td>\n",
       "      <td>0.848556</td>\n",
       "      <td>0.850380</td>\n",
       "      <td>0.808482</td>\n",
       "      <td>0.807444</td>\n",
       "      <td>0.781889</td>\n",
       "      <td>0.748598</td>\n",
       "      <td>0.876331</td>\n",
       "      <td>0.848671</td>\n",
       "      <td>0.850602</td>\n",
       "      <td>9.653419</td>\n",
       "      <td>0.807364</td>\n",
       "      <td>10.273513</td>\n",
       "      <td>0.747972</td>\n",
       "      <td>0.877001</td>\n",
       "      <td>0.848438</td>\n",
       "      <td>0.850380</td>\n",
       "      <td>0.618899</td>\n",
       "      <td>0.807444</td>\n",
       "      <td>0.660471</td>\n",
       "      <td>0.748598</td>\n",
       "      <td>0.876331</td>\n",
       "      <td>0.848556</td>\n",
       "      <td>0.850602</td>\n",
       "      <td>9.653419</td>\n",
       "      <td>0.807444</td>\n",
       "      <td>10.273513</td>\n",
       "      <td>0.748598</td>\n",
       "      <td>0.877001</td>\n",
       "      <td>0.848671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "positive_train_df = train_df[train_df['is_duplicate'] == 1]\n",
    "\n",
    "train_examples_mnrl = [InputExample(texts=[row['question1'], row['question2']])\n",
    "                       for index, row in positive_train_df.iterrows()]\n",
    "\n",
    "train_dataloader_mnrl = DataLoader(train_examples_mnrl, shuffle=True, batch_size=32)\n",
    "\n",
    "mnrl_loss = MultipleNegativesRankingLoss(model=bi_encoder)\n",
    "\n",
    "bi_encoder.fit(\n",
    "    train_objectives=[(train_dataloader_mnrl, mnrl_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=1,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=100,\n",
    "    output_path='output/bi_encoder_mnrl',\n",
    "    use_amp=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a85db1f4-76b8-443d-a3f8-3ed33e46a8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dff8543c1134a9bb713754a2fbc84a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9956512edca24336bbca15dec68848a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224c47daa9a14e899a73593f0eda4a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf39d4dfd6ec47a8a44a34d5bb7fe02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb88207aff14e8d9ff6c0508767e283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c980380566e340469fb0b157b64bb951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder_model_name = 'cross-encoder/quora-roberta-base'\n",
    "cross_encoder = CrossEncoder(cross_encoder_model_name, num_labels=1, device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4b1c601-1727-49d1-8229-e2c09b64a68d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'al_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m cross_encoder \u001b[38;5;241m=\u001b[39m CrossEncoder(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross-encoder/ms-marco-MiniLM-L-6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Prepare the evaluator\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m CEBinaryClassificationEvaluator(val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\u001b[43mal_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_duplicate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m cross_encoder\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     16\u001b[0m     train_data\u001b[38;5;241m=\u001b[39mtrain_samples,\n\u001b[1;32m     17\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39mevaluator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     use_amp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'al_df' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "\n",
    "# Prepare training data\n",
    "train_samples = [(row['question1'], row['question2'], row['is_duplicate'])\n",
    "                 for index, row in train_df.iterrows()]\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\",num_labels=1, device='cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3da3600e-a839-4b1b-ac0b-757cba1cbb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58d1af644f5410dbc4a4f09f3a63056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd76c8042a8484881dc18ee56c55d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/20215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator = CEBinaryClassificationEvaluator(\n",
    "    sentence_pairs=list(zip(val_df['question1'].tolist(), val_df['question2'].tolist())),\n",
    "    labels=val_df['is_duplicate'].tolist()\n",
    ")\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_samples = [(row['question1'], row['question2'], row['is_duplicate'])\n",
    "                 for index, row in train_df.iterrows()]\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "# Train the model\n",
    "cross_encoder.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=1,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=100,\n",
    "    output_path='output/cross_encoder',\n",
    "    use_amp=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd2ae11-9841-4d5e-892d-19434d106d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f1884d62c3451aba953823a6d545ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8587821b684f13acfe6756bbdf5ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c03b95df2a4b909f52f297ce9e61a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ee058854954b23b73364cd779bb324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d338444b61646b8a8d59ce98e973097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e96dc555b8a4c65882c7f7e117b692f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587de9d804144a66a9b0a1673d88e6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_df, is_cross_encoder=False):\n",
    "    if not is_cross_encoder:\n",
    "        embeddings1 = model.encode(test_df['question1'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "        embeddings2 = model.encode(test_df['question2'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "        cosine_scores = torch.nn.functional.cosine_similarity(embeddings1, embeddings2).cpu().numpy()\n",
    "        threshold = 0.5\n",
    "        predictions = [1 if score > threshold else 0 for score in cosine_scores]\n",
    "    else:\n",
    "        scores = model.predict(list(zip(test_df['question1'].tolist(), test_df['question2'].tolist())), show_progress_bar=True)\n",
    "        scores = np.array(scores)\n",
    "        probabilities = 1 / (1 + np.exp(-scores))\n",
    "        threshold = 0.5\n",
    "        predictions = [1 if prob > threshold else 0 for prob in probabilities]\n",
    "\n",
    "    f1 = f1_score(test_df['is_duplicate'].tolist(), predictions)\n",
    "    return f1\n",
    "\n",
    "bi_encoder_base = SentenceTransformer(model_name, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "f1_base = evaluate_model(bi_encoder_base, test_df)\n",
    "\n",
    "bi_encoder_cosine = SentenceTransformer('output/bi_encoder_cosine', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "f1_bi_cosine = evaluate_model(bi_encoder_cosine, test_df)\n",
    "\n",
    "bi_encoder_contrastive = SentenceTransformer('output/bi_encoder_contrastive', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "f1_bi_contrastive = evaluate_model(bi_encoder_contrastive, test_df)\n",
    "\n",
    "bi_encoder_mnrl = SentenceTransformer('output/bi_encoder_mnrl', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "f1_bi_mnrl = evaluate_model(bi_encoder_mnrl, test_df)\n",
    "\n",
    "cross_encoder = CrossEncoder('output/cross_encoder', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "f1_cross = evaluate_model(cross_encoder, test_df, is_cross_encoder=True)\n",
    "\n",
    "print(f\"Off-the-shelf Bi-encoder (Unfine-tuned) F1-Score: {f1_base:.4f}\")\n",
    "print(f\"Bi-encoder with Cosine Similarity Loss F1-Score: {f1_bi_cosine:.4f}\")\n",
    "print(f\"Bi-encoder with Contrastive Loss F1-Score: {f1_bi_contrastive:.4f}\")\n",
    "print(f\"Bi-encoder with Multiple Negative Ranking Loss F1-Score: {f1_bi_mnrl:.4f}\")\n",
    "print(f\"Cross-encoder F1-Score: {f1_cross:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a229a-1d8c-4dc4-a494-178603bd97bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
