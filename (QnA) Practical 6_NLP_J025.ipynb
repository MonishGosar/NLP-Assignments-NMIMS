{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nfrom transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering, Trainer, TrainingArguments, pipeline\nfrom datasets import Dataset\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:46:56.874349Z","iopub.execute_input":"2024-09-02T07:46:56.875094Z","iopub.status.idle":"2024-09-02T07:46:58.448831Z","shell.execute_reply.started":"2024-09-02T07:46:56.875057Z","shell.execute_reply":"2024-09-02T07:46:58.448042Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/stanford-question-answering-dataset/train-v1.1.json') as train_file:\n    train = json.load(train_file)\n\nwith open('/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json') as dev_file:\n    dev = json.load(dev_file)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:46:58.450305Z","iopub.execute_input":"2024-09-02T07:46:58.450973Z","iopub.status.idle":"2024-09-02T07:47:00.038972Z","shell.execute_reply.started":"2024-09-02T07:46:58.450937Z","shell.execute_reply":"2024-09-02T07:47:00.038135Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(data):\n    contexts = []\n    questions = []\n    answers = []\n\n    for article in data['data']:\n        for paragraph in article['paragraphs']:\n            context = paragraph['context']\n            for qa in paragraph['qas']:\n                question = qa['question']\n                answer = qa['answers'][0]  \n                answer['text'] = answer['text']\n                answer['answer_start'] = answer['answer_start']\n\n                contexts.append(context)\n                questions.append(question)\n                answers.append(answer)\n    \n    return Dataset.from_dict({'context': contexts, 'question': questions, 'answers': answers})\n\ntrain = prepare_dataset(train)\ndev = prepare_dataset(dev)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:47:00.040119Z","iopub.execute_input":"2024-09-02T07:47:00.040470Z","iopub.status.idle":"2024-09-02T07:47:01.100278Z","shell.execute_reply.started":"2024-09-02T07:47:00.040437Z","shell.execute_reply":"2024-09-02T07:47:01.099501Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:47:01.102330Z","iopub.execute_input":"2024-09-02T07:47:01.102638Z","iopub.status.idle":"2024-09-02T07:47:01.112198Z","shell.execute_reply.started":"2024-09-02T07:47:01.102606Z","shell.execute_reply":"2024-09-02T07:47:01.111306Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n 'answers': {'answer_start': 515, 'text': 'Saint Bernadette Soubirous'}}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:47:01.113334Z","iopub.execute_input":"2024-09-02T07:47:01.113626Z","iopub.status.idle":"2024-09-02T07:47:02.695805Z","shell.execute_reply.started":"2024-09-02T07:47:01.113593Z","shell.execute_reply":"2024-09-02T07:47:02.694855Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b637b136904f2ab0b9edbcf668309b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4226291ef2e4087a88aa99cfe857458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fdf0d9ac67c4a5a9ae7cc3ecd104e9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d47348d86bc45459cd19cd3716934ad"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    questions = [q.strip() for q in examples['question']]\n    inputs = tokenizer(\n        questions,\n        examples['context'],\n        max_length=384,\n        truncation=True,\n        padding=\"max_length\",\n        return_offsets_mapping=True,  \n        return_tensors=\"pt\"\n    )\n    \n    start_positions = []\n    end_positions = []\n    \n    for i, answer in enumerate(examples['answers']):\n        start_positions.append(answer['answer_start'])\n        end_positions.append(answer['answer_start'] + len(answer['text']))\n    \n    inputs.update({\n        \"start_positions\": start_positions,\n        \"end_positions\": end_positions,\n    })\n    \n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:47:02.697052Z","iopub.execute_input":"2024-09-02T07:47:02.697678Z","iopub.status.idle":"2024-09-02T07:47:02.704109Z","shell.execute_reply.started":"2024-09-02T07:47:02.697642Z","shell.execute_reply":"2024-09-02T07:47:02.703239Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"tokenized_train_dataset = train.map(preprocess_function, batched=True)\ntokenized_dev_dataset = dev.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:47:02.705366Z","iopub.execute_input":"2024-09-02T07:47:02.705648Z","iopub.status.idle":"2024-09-02T07:52:55.680438Z","shell.execute_reply.started":"2024-09-02T07:47:02.705613Z","shell.execute_reply":"2024-09-02T07:52:55.679444Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d686e8b64024ce2a7a7e063a5874c9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adc885db2278400faa965424ff9e10e6"}},"metadata":{}}]},{"cell_type":"code","source":"model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:52:55.681689Z","iopub.execute_input":"2024-09-02T07:52:55.681995Z","iopub.status.idle":"2024-09-02T07:52:57.108382Z","shell.execute_reply.started":"2024-09-02T07:52:55.681963Z","shell.execute_reply":"2024-09-02T07:52:57.107452Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47adb75657984383a4d2830a60bbcba2"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    learning_rate=5e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    fp16=True,  \n    gradient_accumulation_steps=2,\n    dataloader_num_workers=2,  \n    dataloader_pin_memory=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:52:57.109686Z","iopub.execute_input":"2024-09-02T07:52:57.110003Z","iopub.status.idle":"2024-09-02T07:52:57.234793Z","shell.execute_reply.started":"2024-09-02T07:52:57.109971Z","shell.execute_reply":"2024-09-02T07:52:57.233839Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_dev_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:52:57.237948Z","iopub.execute_input":"2024-09-02T07:52:57.238587Z","iopub.status.idle":"2024-09-02T07:52:58.257034Z","shell.execute_reply.started":"2024-09-02T07:52:57.238554Z","shell.execute_reply":"2024-09-02T07:52:58.256256Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_iou(predictions, references):\n    ious = []\n    for pred, ref in zip(predictions, references):\n        pred_tokens = set(range(pred['start_positions'], pred['end_positions']))\n        ref_tokens = set(range(ref['start_positions'], ref['end_positions']))\n        intersection = len(pred_tokens & ref_tokens)\n        union = len(pred_tokens | ref_tokens)\n        iou = intersection / union if union != 0 else 0\n        ious.append(iou)\n    return {\"token_level_iou\": np.mean(ious)}","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:52:58.258160Z","iopub.execute_input":"2024-09-02T07:52:58.258497Z","iopub.status.idle":"2024-09-02T07:52:58.264366Z","shell.execute_reply.started":"2024-09-02T07:52:58.258465Z","shell.execute_reply":"2024-09-02T07:52:58.263408Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    pred_starts = predictions[0].argmax(-1)\n    pred_ends = predictions[1].argmax(-1)\n    label_starts = labels['start_positions']\n    label_ends = labels['end_positions']\n\n    pred = [{'start_positions': start, 'end_positions': end} for start, end in zip(pred_starts, pred_ends)]\n    ref = [{'start_positions': start, 'end_positions': end} for start, end in zip(label_starts, label_ends)]\n\n    return compute_iou(pred, ref)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:52:58.265677Z","iopub.execute_input":"2024-09-02T07:52:58.265933Z","iopub.status.idle":"2024-09-02T07:52:58.276024Z","shell.execute_reply.started":"2024-09-02T07:52:58.265904Z","shell.execute_reply":"2024-09-02T07:52:58.275259Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-02T07:52:58.277025Z","iopub.execute_input":"2024-09-02T07:52:58.277354Z","iopub.status.idle":"2024-09-02T09:05:38.682520Z","shell.execute_reply.started":"2024-09-02T07:52:58.277313Z","shell.execute_reply":"2024-09-02T09:05:38.681542Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240902_081450-ujenltr2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/monish-emailbox-svkm-s-narsee-monjee-institute-of-manage/huggingface/runs/ujenltr2' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/monish-emailbox-svkm-s-narsee-monjee-institute-of-manage/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/monish-emailbox-svkm-s-narsee-monjee-institute-of-manage/huggingface' target=\"_blank\">https://wandb.ai/monish-emailbox-svkm-s-narsee-monjee-institute-of-manage/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/monish-emailbox-svkm-s-narsee-monjee-institute-of-manage/huggingface/runs/ujenltr2' target=\"_blank\">https://wandb.ai/monish-emailbox-svkm-s-narsee-monjee-institute-of-manage/huggingface/runs/ujenltr2</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1368' max='1368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1368/1368 50:27, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>5.595300</td>\n      <td>5.261881</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>5.152200</td>\n      <td>5.045757</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1368, training_loss=5.272233483387016, metrics={'train_runtime': 4359.7589, 'train_samples_per_second': 40.185, 'train_steps_per_second': 0.314, 'total_flos': 1.7156744493977088e+16, 'train_loss': 5.272233483387016, 'epoch': 1.9985390796201608})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2024-09-02T09:05:38.747980Z","iopub.execute_input":"2024-09-02T09:05:38.748365Z","iopub.status.idle":"2024-09-02T09:05:39.363705Z","shell.execute_reply.started":"2024-09-02T09:05:38.748318Z","shell.execute_reply":"2024-09-02T09:05:39.362564Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nqa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T09:05:38.683973Z","iopub.execute_input":"2024-09-02T09:05:38.684327Z","iopub.status.idle":"2024-09-02T09:05:38.691584Z","shell.execute_reply.started":"2024-09-02T09:05:38.684291Z","shell.execute_reply":"2024-09-02T09:05:38.690521Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"context = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge.\"\nquestion = \"Where is Hugging Face based?\"\n\nresult = qa({\n    'context': context,\n    'question': question\n})\n\nprint(\"Prediction:\", result)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T09:05:38.692791Z","iopub.execute_input":"2024-09-02T09:05:38.693078Z","iopub.status.idle":"2024-09-02T09:05:38.744641Z","shell.execute_reply.started":"2024-09-02T09:05:38.693046Z","shell.execute_reply":"2024-09-02T09:05:38.743266Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Prediction: {'score': 0.001387766096740961, 'start': 76, 'end': 131, 'answer': 'in DUMBO, therefore very close to the Manhattan Bridge.'}\n","output_type":"stream"}]},{"cell_type":"code","source":"context = \"My name is Monish Gosar. I study in NMIMS.\"\nquestion = \"Where does monish study?\"\n\nresult = qa({\n    'context': context,\n    'question': question\n})\n\nprint(\"Prediction:\", result)","metadata":{"execution":{"iopub.status.busy":"2024-09-02T09:23:35.607341Z","iopub.execute_input":"2024-09-02T09:23:35.608359Z","iopub.status.idle":"2024-09-02T09:23:35.633247Z","shell.execute_reply.started":"2024-09-02T09:23:35.608308Z","shell.execute_reply":"2024-09-02T09:23:35.632256Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Prediction: {'score': 0.012747673317790031, 'start': 0, 'end': 42, 'answer': 'My name is Monish Gosar. I study in NMIMS.'}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}