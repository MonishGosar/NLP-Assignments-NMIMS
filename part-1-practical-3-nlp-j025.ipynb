{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":9294862,"sourceType":"datasetVersion","datasetId":5627421}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nimport string\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:59:04.256637Z","iopub.execute_input":"2024-09-09T12:59:04.257251Z","iopub.status.idle":"2024-09-09T12:59:10.062843Z","shell.execute_reply.started":"2024-09-09T12:59:04.257211Z","shell.execute_reply":"2024-09-09T12:59:10.061984Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:59:10.064517Z","iopub.execute_input":"2024-09-09T12:59:10.064967Z","iopub.status.idle":"2024-09-09T12:59:10.233436Z","shell.execute_reply.started":"2024-09-09T12:59:10.064916Z","shell.execute_reply":"2024-09-09T12:59:10.232396Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:59:10.234464Z","iopub.execute_input":"2024-09-09T12:59:10.234785Z","iopub.status.idle":"2024-09-09T12:59:11.513940Z","shell.execute_reply.started":"2024-09-09T12:59:10.234751Z","shell.execute_reply":"2024-09-09T12:59:11.513093Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nstop_words = set(stopwords.words('english')).union(set(ENGLISH_STOP_WORDS))\npunctuation = string.punctuation\n\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    filtered_tokens = [word for word in tokens if word not in stop_words and word not in punctuation]\n    return ' '.join(filtered_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:59:11.516230Z","iopub.execute_input":"2024-09-09T12:59:11.516587Z","iopub.status.idle":"2024-09-09T12:59:11.525208Z","shell.execute_reply.started":"2024-09-09T12:59:11.516551Z","shell.execute_reply":"2024-09-09T12:59:11.524320Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df['preprocessed_review'] = df['review'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T12:59:11.526848Z","iopub.execute_input":"2024-09-09T12:59:11.527187Z","iopub.status.idle":"2024-09-09T13:01:34.746068Z","shell.execute_reply.started":"2024-09-09T12:59:11.527130Z","shell.execute_reply":"2024-09-09T13:01:34.744899Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"glove_path = '/kaggle/input/glove-6b-100d/glove.6B.100d.txt'  \nembeddings_index = {}\n\nwith open(glove_path, 'r', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = vector\n\ndef get_review_embedding(review):\n    words = review.split()\n    vectors = [embeddings_index.get(word) for word in words if word in embeddings_index]\n    if not vectors:  \n        return np.zeros(100)  \n    return np.mean(vectors, axis=0)\n\ndf['embedding'] = df['preprocessed_review'].apply(get_review_embedding)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:01:34.747445Z","iopub.execute_input":"2024-09-09T13:01:34.747790Z","iopub.status.idle":"2024-09-09T13:01:53.711601Z","shell.execute_reply.started":"2024-09-09T13:01:34.747757Z","shell.execute_reply":"2024-09-09T13:01:53.710593Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\nX = np.stack(df['embedding'].values)\ny = df['sentiment'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:01:53.712914Z","iopub.execute_input":"2024-09-09T13:01:53.713248Z","iopub.status.idle":"2024-09-09T13:01:53.851925Z","shell.execute_reply.started":"2024-09-09T13:01:53.713215Z","shell.execute_reply":"2024-09-09T13:01:53.851015Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# RNN Using Glove","metadata":{}},{"cell_type":"code","source":"class SimpleRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        h0 = torch.zeros(1, x.size(0), self.hidden_size) \n        out, _ = self.rnn(x.unsqueeze(1), h0) \n        out = self.fc(out[:, -1, :])  \n        return out\n\ninput_size = 100  \nhidden_size = 50  \noutput_size = 2   \nlearning_rate = 0.01\nnum_epochs = 10\n\nmodel = SimpleRNN(input_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for i, (inputs, labels) in enumerate(train_loader):\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print(f'Test Accuracy: {100 * correct / total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:01:53.853255Z","iopub.execute_input":"2024-09-09T13:01:53.853666Z","iopub.status.idle":"2024-09-09T13:07:05.214364Z","shell.execute_reply.started":"2024-09-09T13:01:53.853621Z","shell.execute_reply":"2024-09-09T13:07:05.213327Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.7948\nEpoch [2/10], Loss: 0.8101\nEpoch [3/10], Loss: 1.4197\nEpoch [4/10], Loss: 0.1160\nEpoch [5/10], Loss: 0.3032\nEpoch [6/10], Loss: 0.6656\nEpoch [7/10], Loss: 0.3414\nEpoch [8/10], Loss: 3.2027\nEpoch [9/10], Loss: 0.1772\nEpoch [10/10], Loss: 1.1844\nTest Accuracy: 73.27%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LSTM Using Glove","metadata":{}},{"cell_type":"code","source":"class SimpleLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n        super(SimpleLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  \n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  \n        \n        out, _ = self.lstm(x.unsqueeze(1), (h0, c0))  \n        out = self.fc(out[:, -1, :])  \n        return out\n\ninput_size = 100  \nhidden_size = 50 \noutput_size = 2   \nlearning_rate = 0.01\nnum_epochs = 10\nnum_layers = 1    \n\n\nmodel = SimpleLSTM(input_size, hidden_size, output_size, num_layers)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for i, (inputs, labels) in enumerate(train_loader):\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print(f'Test Accuracy: {100 * correct / total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:07:05.216113Z","iopub.execute_input":"2024-09-09T13:07:05.216714Z","iopub.status.idle":"2024-09-09T13:14:05.564503Z","shell.execute_reply.started":"2024-09-09T13:07:05.216666Z","shell.execute_reply":"2024-09-09T13:14:05.563473Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.7593\nEpoch [2/10], Loss: 0.8521\nEpoch [3/10], Loss: 0.2136\nEpoch [4/10], Loss: 0.1681\nEpoch [5/10], Loss: 0.0658\nEpoch [6/10], Loss: 0.1257\nEpoch [7/10], Loss: 0.3455\nEpoch [8/10], Loss: 0.3598\nEpoch [9/10], Loss: 0.0448\nEpoch [10/10], Loss: 0.6817\nTest Accuracy: 78.03%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RNN and LSTM Using On-Fly Embeddings","metadata":{}},{"cell_type":"code","source":"import nltk\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nimport string\n\nnltk.download('punkt')\n\ndf = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf = df.sample(frac=0.1, random_state=42)\n\ndef preprocess(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [word for word in tokens if word.isalpha()]\n    return tokens\n\ndf['processed'] = df['review'].apply(preprocess)\n\nprint(\"Preprocessing completed. Sample of processed reviews:\")\nprint(df['processed'].head())","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:14:05.568283Z","iopub.execute_input":"2024-09-09T13:14:05.568653Z","iopub.status.idle":"2024-09-09T13:14:20.509525Z","shell.execute_reply.started":"2024-09-09T13:14:05.568620Z","shell.execute_reply":"2024-09-09T13:14:20.508518Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nPreprocessing completed. Sample of processed reviews:\n33553    [i, really, liked, this, summerslam, due, to, ...\n9427     [not, many, television, shows, appeal, to, qui...\n199      [the, film, quickly, gets, to, a, major, chase...\n12447    [jane, austen, would, definitely, approve, of,...\n39489    [expectations, were, somewhat, high, for, me, ...\nName: processed, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab = {}\nword_count = 1\nencoded_reviews = []\n\nfor review in df['processed']:\n    encoded_review = []\n    for word in review:\n        if word not in vocab:\n            vocab[word] = word_count\n            word_count += 1\n        encoded_review.append(vocab[word])\n    encoded_reviews.append(encoded_review)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:14:20.510722Z","iopub.execute_input":"2024-09-09T13:14:20.511044Z","iopub.status.idle":"2024-09-09T13:14:20.957443Z","shell.execute_reply.started":"2024-09-09T13:14:20.511009Z","shell.execute_reply":"2024-09-09T13:14:20.956421Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"max_length = max(len(review) for review in encoded_reviews)\nencoded_reviews = [review + [0] * (max_length - len(review)) for review in encoded_reviews]\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:14:20.958839Z","iopub.execute_input":"2024-09-09T13:14:20.959632Z","iopub.status.idle":"2024-09-09T13:14:21.054967Z","shell.execute_reply.started":"2024-09-09T13:14:20.959567Z","shell.execute_reply":"2024-09-09T13:14:21.054164Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X = torch.tensor(encoded_reviews, dtype=torch.long)\ny = torch.tensor(df['sentiment'].values, dtype=torch.long)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntest_dataset = TensorDataset(X_test, y_test)\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:14:21.056055Z","iopub.execute_input":"2024-09-09T13:14:21.056405Z","iopub.status.idle":"2024-09-09T13:14:21.828655Z","shell.execute_reply.started":"2024-09-09T13:14:21.056368Z","shell.execute_reply":"2024-09-09T13:14:21.827668Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class RNNWithEmbedding(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=1):\n        super(RNNWithEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n        out, _ = self.rnn(x, h0)\n        out = self.fc(out[:, -1, :])  \n        return out\n\n# Hyperparameters\nvocab_size = len(vocab) + 1  \nembedding_dim = 100\nhidden_size = 50\noutput_size = 2  \nlearning_rate = 0.01\nnum_epochs = 10\nnum_layers = 1\n\nmodel = RNNWithEmbedding(vocab_size, embedding_dim, hidden_size, output_size, num_layers)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    model.train()\n    for i, (inputs, labels) in enumerate(train_loader):\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print(f'Test Accuracy: {100 * correct / total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:14:21.829894Z","iopub.execute_input":"2024-09-09T13:14:21.830194Z","iopub.status.idle":"2024-09-09T13:54:36.519340Z","shell.execute_reply.started":"2024-09-09T13:14:21.830162Z","shell.execute_reply":"2024-09-09T13:54:36.518286Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 1.0959\nEpoch [2/10], Loss: 0.1807\nEpoch [3/10], Loss: 0.7644\nEpoch [4/10], Loss: 0.7001\nEpoch [5/10], Loss: 0.7305\nEpoch [6/10], Loss: 0.9094\nEpoch [7/10], Loss: 0.6934\nEpoch [8/10], Loss: 0.4596\nEpoch [9/10], Loss: 0.5571\nEpoch [10/10], Loss: 0.8626\nTest Accuracy: 50.60%\n","output_type":"stream"}]},{"cell_type":"code","source":"class LSTMWithEmbedding(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=1):\n        super(LSTMWithEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  # Hidden state\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)  # Cell state\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :]) \n        return out\n\n# Hyperparameters\nvocab_size = len(vocab) + 1  \nembedding_dim = 100\nhidden_size = 50\noutput_size = 2  \nlearning_rate = 0.01\nnum_epochs = 10\nnum_layers = 1\n\nmodel = LSTMWithEmbedding(vocab_size, embedding_dim, hidden_size, output_size, num_layers)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for i, (inputs, labels) in enumerate(train_loader):\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print(f'Test Accuracy: {100 * correct / total:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:54:36.520963Z","iopub.execute_input":"2024-09-09T13:54:36.521279Z","iopub.status.idle":"2024-09-09T14:33:30.192353Z","shell.execute_reply.started":"2024-09-09T13:54:36.521246Z","shell.execute_reply":"2024-09-09T14:33:30.191266Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.6949\nEpoch [2/10], Loss: 0.6338\nEpoch [3/10], Loss: 0.6854\nEpoch [4/10], Loss: 0.7536\nEpoch [5/10], Loss: 0.8409\nEpoch [6/10], Loss: 0.3883\nEpoch [7/10], Loss: 0.5723\nEpoch [8/10], Loss: 0.3564\nEpoch [9/10], Loss: 0.1354\nEpoch [10/10], Loss: 0.0889\nTest Accuracy: 59.10%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}